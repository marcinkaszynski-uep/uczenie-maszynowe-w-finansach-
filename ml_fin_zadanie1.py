# -*- coding: utf-8 -*-
"""ML_fin_zadanie1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ukDn_7ytrOB_A4_GvBTvvI0amjiaPNQo
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, precision_recall_curve
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.utils import resample
from imblearn.over_sampling import RandomOverSampler, SMOTE

credit_df = pd.read_csv('creditcard.csv')
credit_df.head()

credit_df.info()

# dzielenie danych i Standaryzacja
X = credit_df.drop('Class', axis=1)
y = credit_df['Class']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# redukcja wymiarów
pca = PCA(n_components=0.95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print(np.shape(X_test_pca))

train_df = pd.concat([
    pd.DataFrame(X_train_pca),
    pd.Series(y_train.reset_index(drop=True), name='Class')
], axis=1)

#  undersampling
fraud = train_df[train_df['Class'] == 1]
normal = train_df[train_df['Class'] == 0]

normal_sampled = resample(normal,
                          replace=False,
                          n_samples=len(fraud) * 20,
                          random_state=42)

df_under = pd.concat([fraud, normal_sampled]).sample(frac=1, random_state=42)
X_train_under = df_under.drop('Class', axis=1)
y_train_under = df_under['Class']

print(np.shape(X_train_under))

# Oversampling

ros = RandomOverSampler(sampling_strategy=0.05, random_state=42)
X_train_over, y_train_over = ros.fit_resample(X_train_pca, y_train)

# SMOTE

smote = SMOTE(sampling_strategy=0.05, random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_pca, y_train)

# model regresji logistycznej z undersamplingiem
logreg_under = LogisticRegression()
logreg_under.fit(X_train_under, y_train_under)
y_pred_under_probs = logreg_under.predict_proba(X_test_pca)[:,1]

print('model regresji logistycznej z undersamplingiem')
print(confusion_matrix(y_test, logreg_under.predict(X_test_pca)))
print(classification_report(y_test, logreg_under.predict(X_test_pca), digits=4))
print(roc_auc_score(y_test, y_pred_under_probs))

# model regresji logistycznej z oversamplingiem

logreg_over = LogisticRegression()
logreg_over.fit(X_train_over, y_train_over)
logreg_over.predict(X_test_pca)
y_pred_over_probs = logreg_over.predict_proba(X_test_pca)[:,1]

print('model regresji logistycznej z oversamplingiem')
print(confusion_matrix(y_test, logreg_over.predict(X_test_pca)))
print(classification_report(y_test, logreg_over.predict(X_test_pca), digits=4))
print(roc_auc_score(y_test, y_pred_over_probs))

# model regresji logistycznej ze SMOTE

logreg_smote = LogisticRegression()
logreg_smote.fit(X_train_smote, y_train_smote)
logreg_smote.predict(X_test_pca)
y_pred_smote_probs = logreg_smote.predict_proba(X_test_pca)[:,1]


print('model regresji logistycznej ze SMOTE')
print(confusion_matrix(y_test, logreg_smote.predict(X_test_pca)))
print(classification_report(y_test, logreg_smote.predict(X_test_pca), digits=4))
print(roc_auc_score(y_test, y_pred_smote_probs))

# Wykresy ROC
models = {
    'Undersampling': (logreg_under, y_pred_under_probs),
    'Oversampling': (logreg_over, y_pred_over_probs),
    'SMOTE': (logreg_smote, y_pred_smote_probs)
}

for name, (model, probs) in models.items():
    fpr, tpr, thresholds = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=name)

plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Porównanie ROC dla trzech metod balansowania')
plt.legend()
plt.grid(True)
plt.show()

'''
Przy wykrywaniu fraudów najistotniejszym wynikiem jest recall dla zmiennej 1,
gdze wszystkie modele radzą sobie tak samo dobrze, biorąc pod uwagę inne
prametry najlepiej wypadł model z bilansowaniem SMOTE, który ma najlepszy f1 score
ze względu na najmniejszą ilość fałszywych pozytywów (najlepsza precyzja).
'''